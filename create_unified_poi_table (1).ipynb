{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb129213",
   "metadata": {},
   "source": [
    "# Creating the unified POI table\n",
    "- Used notebook to start so I can see the output and use markdwon to make notes as I go along\n",
    "- Once done will copy accross to a .py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef63781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Librabies\n",
    "import osmnx as ox # to fetch data from OpenStreetMap\n",
    "import geopandas as gpd # to work with geospatial data\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Credentials\n",
    "user_name=''\n",
    "password=''\n",
    "\n",
    "# Conection\n",
    "host = 'localhost'\n",
    "port = '5433'\n",
    "database = 'layereddb'\n",
    "schema='berlin_source_data'\n",
    "\n",
    "# Connection to DB\n",
    "engine = create_engine(f'postgresql+psycopg2://{user_name}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab07c4",
   "metadata": {},
   "source": [
    "## Step 1 - Creating missing data table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    DROP TABLE IF EXISTS excluded_tables_log CASCADE;\n",
    "\n",
    "    CREATE TABLE excluded_tables_log (\n",
    "        table_name VARCHAR(255),\n",
    "        reason VARCHAR(500),\n",
    "        exclusion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    WITH all_tables AS (\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'berlin_source_data' \n",
    "        AND table_name NOT ILIKE '%districts%'\n",
    "        AND table_name NOT ILIKE '%neighborhoods%'\n",
    "    ),\n",
    "\n",
    "    -- Required schema and constraints\n",
    "    expected_schema AS (\n",
    "        SELECT * FROM (VALUES\n",
    "            ('id', 'character varying', NULL, 'PRIMARY KEY', 'NO'),\n",
    "            ('district_id', 'character varying', NULL, 'FOREIGN KEY', 'NO'),\n",
    "            ('name', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('latitude', 'numeric', NULL, NULL, 'YES'),\n",
    "            ('longitude', 'numeric', NULL, NULL, 'YES'),\n",
    "            ('neighborhood', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('district', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('neighborhood_id', 'character varying', NULL, NULL, 'YES')\n",
    "        ) AS t(column_name, expected_type, expected_length, constraint_type, is_nullable)\n",
    "    ),\n",
    "\n",
    "    table_columns AS (\n",
    "        SELECT \n",
    "            t.table_name,\n",
    "            COUNT(CASE WHEN LOWER(c.column_name) IN (\n",
    "                'id','name','district_id','district','latitude','longitude','neighborhood_id','neighborhood','geometry'\n",
    "            ) THEN 1 END) AS required_col_count\n",
    "        FROM all_tables t\n",
    "        LEFT JOIN information_schema.columns c \n",
    "            ON t.table_name = c.table_name \n",
    "        AND c.table_schema = 'berlin_source_data'\n",
    "        GROUP BY t.table_name\n",
    "    ),\n",
    "\n",
    "    missing_columns AS (\n",
    "        SELECT \n",
    "            tc.table_name,\n",
    "            STRING_AGG(DISTINCT col, ', ' ORDER BY col) AS missing_cols\n",
    "        FROM table_columns tc\n",
    "        CROSS JOIN (VALUES \n",
    "            ('id'), ('name'), ('district_id'), ('district'),\n",
    "            ('latitude'), ('longitude'), ('neighborhood_id'),\n",
    "            ('neighborhood'), ('geometry')\n",
    "        ) AS req_cols(col)\n",
    "        LEFT JOIN information_schema.columns c \n",
    "            ON tc.table_name = c.table_name \n",
    "        AND c.table_schema = 'berlin_source_data'\n",
    "        AND LOWER(c.column_name) = req_cols.col\n",
    "        WHERE c.column_name IS NULL\n",
    "        GROUP BY tc.table_name\n",
    "        HAVING COUNT(*) > 0\n",
    "    ),\n",
    "\n",
    "    -- ❗ NEW: Data type checks (VARCHAR length ignored)\n",
    "    datatype_issues AS (\n",
    "        SELECT \n",
    "            c.table_name,\n",
    "            e.column_name,\n",
    "            'Expected data type ' || e.expected_type || ', got ' || c.data_type AS reason\n",
    "        FROM information_schema.columns c\n",
    "        JOIN expected_schema e \n",
    "            ON LOWER(c.column_name) = e.column_name\n",
    "        WHERE c.table_schema = 'berlin_source_data'\n",
    "        AND (\n",
    "                c.data_type <> e.expected_type\n",
    "                OR (\n",
    "                    c.data_type = 'numeric'\n",
    "                    AND (c.numeric_precision || ',' || c.numeric_scale) <> e.expected_length\n",
    "                )\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    -- ❗ NEW: NULL constraint issues\n",
    "    null_issues AS (\n",
    "        SELECT \n",
    "            c.table_name,\n",
    "            'Column ' || c.column_name || ' allows NULL, expected NOT NULL' AS reason\n",
    "        FROM information_schema.columns c\n",
    "        JOIN expected_schema e \n",
    "            ON LOWER(c.column_name) = e.column_name\n",
    "        WHERE c.table_schema = 'berlin_source_data'\n",
    "        AND e.is_nullable = 'NO'\n",
    "        AND c.is_nullable = 'YES'\n",
    "    ),\n",
    "\n",
    "    -- ❗ PRIMARY KEY check on id\n",
    "    pk_issues AS (\n",
    "        SELECT \n",
    "            t.table_name,\n",
    "            'Missing PRIMARY KEY on id column' AS reason\n",
    "        FROM all_tables t\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "            ON kcu.constraint_name = tc.constraint_name\n",
    "            AND kcu.table_name = tc.table_name\n",
    "            WHERE tc.table_schema = 'berlin_source_data'\n",
    "            AND tc.table_name = t.table_name\n",
    "            AND tc.constraint_type = 'PRIMARY KEY'\n",
    "            AND LOWER(kcu.column_name) = 'id'\n",
    "        )\n",
    "    ),\n",
    "\n",
    "-- ❗ FOREIGN KEY check on district_id with RESTRICT / CASCADE\n",
    "fk_issues AS (\n",
    "    SELECT t.table_name,\n",
    "           'Missing or incorrect foreign key on district_id (expected → berlin_data.districts(district_id) ON DELETE RESTRICT ON UPDATE CASCADE)' AS reason\n",
    "    FROM all_tables t\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM information_schema.table_constraints tc\n",
    "        JOIN information_schema.key_column_usage kcu\n",
    "          ON kcu.constraint_name = tc.constraint_name\n",
    "         AND kcu.table_name = tc.table_name\n",
    "        JOIN information_schema.referential_constraints rc\n",
    "          ON rc.constraint_name = tc.constraint_name\n",
    "         AND rc.constraint_schema = tc.table_schema\n",
    "        JOIN information_schema.constraint_column_usage ccu\n",
    "          ON ccu.constraint_name = rc.unique_constraint_name\n",
    "         AND ccu.table_schema = 'berlin_source_data'\n",
    "         AND ccu.table_name = 'districts'\n",
    "         AND LOWER(ccu.column_name) = 'district_id'\n",
    "        WHERE tc.table_schema = 'berlin_source_data'\n",
    "          AND tc.table_name = t.table_name\n",
    "          AND tc.constraint_type = 'FOREIGN KEY'\n",
    "          AND tc.constraint_name = 'district_id_fk'  -- <== exact constraint name\n",
    "          AND LOWER(kcu.column_name) = 'district_id'\n",
    "          AND rc.delete_rule = 'RESTRICT'\n",
    "          AND rc.update_rule = 'CASCADE'\n",
    "        LIMIT 1\n",
    "    )\n",
    ")\n",
    "\n",
    "-- INSERT all results\n",
    "INSERT INTO excluded_tables_log (table_name, reason)\n",
    "SELECT table_name, 'Missing columns: ' || missing_cols FROM missing_columns\n",
    "UNION ALL\n",
    "SELECT table_name, reason FROM datatype_issues\n",
    "UNION ALL\n",
    "SELECT table_name, reason FROM null_issues\n",
    "UNION ALL\n",
    "SELECT table_name, reason FROM pk_issues\n",
    "UNION ALL\n",
    "SELECT table_name, reason FROM fk_issues;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510b558",
   "metadata": {},
   "source": [
    "## Step 2 - Creating missing data table, valid table and full unified_pois table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation query complete\n",
      "valid tables complete\n",
      "['food_markets', 'galleries', 'long_term_listings']\n",
      "Unified Table created\n",
      "Inserted 3 tables with computed nearest_pois.\n",
      "Spatial index created.\n"
     ]
    }
   ],
   "source": [
    "# Create the excluded_table_logs table\n",
    "validation_query = \"\"\"\n",
    "DROP TABLE IF EXISTS excluded_tables_log CASCADE;\n",
    "\n",
    "CREATE TABLE excluded_tables_log (\n",
    "    table_name VARCHAR(255),\n",
    "    reason VARCHAR(500),\n",
    "    exclusion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Dont include the districts and neighborhood tables as seperate entities\n",
    "WITH all_tables AS (\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'berlin_source_data'\n",
    "      AND table_name NOT ILIKE '%districts%'\n",
    "      AND table_name NOT ILIKE '%neighborhoods%'\n",
    "),\n",
    "\n",
    "-- What the data types should look like on tables already created\n",
    "expected_schema AS (\n",
    "    SELECT * FROM (VALUES\n",
    "        ('id', 'character varying', NULL, 'PRIMARY KEY', 'NO'),\n",
    "        ('district_id', 'character varying', NULL, 'FOREIGN KEY', 'NO'),\n",
    "        ('name', 'character varying', NULL, NULL, 'YES'),\n",
    "        ('latitude', 'numeric', NULL, NULL, 'YES'),\n",
    "        ('longitude', 'numeric', NULL, NULL, 'YES'),\n",
    "        ('neighborhood', 'character varying', NULL, NULL, 'YES'),\n",
    "        ('district', 'character varying', NULL, NULL, 'YES'),\n",
    "        ('neighborhood_id', 'character varying', NULL, NULL, 'YES')\n",
    "    ) AS t(column_name, expected_type, expected_length, constraint_type, is_nullable)\n",
    "),\n",
    "\n",
    "-- Missing columns checks that table has all columns needed else rejects it\n",
    "missing_columns AS (\n",
    "    SELECT \n",
    "        tc.table_name,\n",
    "        STRING_AGG(DISTINCT col, ', ' ORDER BY col) AS missing_cols\n",
    "    FROM all_tables tc\n",
    "    CROSS JOIN (VALUES \n",
    "        ('id'), ('name'), ('district_id'), ('district'),\n",
    "        ('latitude'), ('longitude'), ('neighborhood_id'),\n",
    "        ('neighborhood'), ('geometry')\n",
    "    ) AS req_cols(col)\n",
    "    LEFT JOIN information_schema.columns c \n",
    "        ON tc.table_name = c.table_name \n",
    "        AND c.table_schema = 'berlin_source_data'\n",
    "        AND LOWER(c.column_name) = req_cols.col\n",
    "    WHERE c.column_name IS NULL\n",
    "    GROUP BY tc.table_name\n",
    "    HAVING COUNT(*) > 0\n",
    "),\n",
    "\n",
    "-- using above schema setup checks the tables in the database have correct data type, nulls, primary key & foreign key\n",
    "-- Datatype issues \n",
    "datatype_issues AS (\n",
    "    SELECT \n",
    "        c.table_name,\n",
    "        'Expected data type ' || e.expected_type || ', got ' || c.data_type AS reason\n",
    "    FROM information_schema.columns c\n",
    "    JOIN expected_schema e \n",
    "        ON LOWER(c.column_name) = e.column_name\n",
    "    WHERE c.table_schema = 'berlin_source_data'\n",
    "      AND (\n",
    "            c.data_type <> e.expected_type\n",
    "            OR (c.data_type = 'numeric' AND (c.numeric_precision || ',' || c.numeric_scale) <> e.expected_length)\n",
    "      )\n",
    "),\n",
    "\n",
    "-- Null issues\n",
    "null_issues AS (\n",
    "    SELECT \n",
    "        c.table_name,\n",
    "        'Column ' || c.column_name || ' allows NULL, expected NOT NULL' AS reason\n",
    "    FROM information_schema.columns c\n",
    "    JOIN expected_schema e \n",
    "        ON LOWER(c.column_name) = e.column_name\n",
    "    WHERE c.table_schema = 'berlin_source_data'\n",
    "      AND e.is_nullable = 'NO'\n",
    "      AND c.is_nullable = 'YES'\n",
    "),\n",
    "\n",
    "-- PK issues\n",
    "pk_issues AS (\n",
    "    SELECT \n",
    "        t.table_name,\n",
    "        'Missing PRIMARY KEY on id column' AS reason\n",
    "    FROM all_tables t\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM information_schema.table_constraints tc\n",
    "        JOIN information_schema.key_column_usage kcu\n",
    "            ON kcu.constraint_name = tc.constraint_name\n",
    "            AND kcu.table_name = tc.table_name\n",
    "        WHERE tc.table_schema = 'berlin_source_data'\n",
    "          AND tc.table_name = t.table_name\n",
    "          AND tc.constraint_type = 'PRIMARY KEY'\n",
    "          AND LOWER(kcu.column_name) = 'id'\n",
    "    )\n",
    "),\n",
    "\n",
    "-- FK issues\n",
    "fk_issues AS (\n",
    "    SELECT t.table_name,\n",
    "           'Missing or incorrect foreign key on district_id (expected → berlin_data.districts(district_id) ON DELETE RESTRICT ON UPDATE CASCADE)' AS reason\n",
    "    FROM all_tables t\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM information_schema.table_constraints tc\n",
    "        JOIN information_schema.key_column_usage kcu\n",
    "          ON kcu.constraint_name = tc.constraint_name\n",
    "         AND kcu.table_name = tc.table_name\n",
    "        JOIN information_schema.referential_constraints rc\n",
    "          ON rc.constraint_name = tc.constraint_name\n",
    "         AND rc.constraint_schema = tc.table_schema\n",
    "        JOIN information_schema.constraint_column_usage ccu\n",
    "          ON ccu.constraint_name = rc.unique_constraint_name\n",
    "         AND ccu.table_schema = 'berlin_source_data'\n",
    "         AND ccu.table_name = 'districts'\n",
    "         AND LOWER(ccu.column_name) = 'district_id'\n",
    "        WHERE tc.table_schema = 'berlin_source_data'\n",
    "          AND tc.table_name = t.table_name\n",
    "          AND tc.constraint_type = 'FOREIGN KEY'\n",
    "          AND tc.constraint_name = 'district_id_fk'\n",
    "          AND LOWER(kcu.column_name) = 'district_id'\n",
    "          AND rc.delete_rule = 'RESTRICT'\n",
    "          AND rc.update_rule = 'CASCADE'\n",
    "        LIMIT 1\n",
    "    )\n",
    "),\n",
    "\n",
    "-- All invalid tables - creates rows from tables that dont match above\n",
    "invalid_tables AS (\n",
    "    SELECT table_name, 'Missing columns: ' || missing_cols AS reason FROM missing_columns\n",
    "    UNION ALL\n",
    "    SELECT table_name, reason FROM datatype_issues\n",
    "    UNION ALL\n",
    "    SELECT table_name, reason FROM null_issues\n",
    "    UNION ALL\n",
    "    SELECT table_name, reason FROM pk_issues\n",
    "    UNION ALL\n",
    "    SELECT table_name, reason FROM fk_issues\n",
    ")\n",
    "\n",
    "-- Insert invalid tables into log\n",
    "INSERT INTO excluded_tables_log (table_name, reason)\n",
    "SELECT table_name, reason FROM invalid_tables;\n",
    "\"\"\"\n",
    "\n",
    "# Runs the above Validation Query \n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(validation_query))\n",
    "\n",
    "print(\"✅ Validation query complete\")\n",
    "\n",
    "# Creates a list of valid tables (excludes districts, neighborhoods and invalid tables) \n",
    "valid_tables_sql = \"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'berlin_source_data'\n",
    "        AND table_name NOT ILIKE '%districts%'\n",
    "        AND table_name NOT ILIKE '%neighborhoods%'\n",
    "        AND table_name NOT IN (SELECT table_name FROM excluded_tables_log)\n",
    "        AND table_name IN ('galleries', 'food_markets', 'long_term_listings')\n",
    "\"\"\"\n",
    "\n",
    "# Runs above valid_tables query \n",
    "with engine.begin() as conn:\n",
    "    valid_tables = [row[0] for row in conn.execute(text(valid_tables_sql))]\n",
    "\n",
    "print(\"valid tables complete\")\n",
    "print(valid_tables)\n",
    "\n",
    "# Drop table if exists (this is just for practice - must be removed)\n",
    "with engine.begin() as conn:\n",
    "   conn.execute(text(\"DROP TABLE IF EXISTS unified_pois CASCADE;\"))\n",
    "\n",
    "# Create the unified_pois table\n",
    "create_unified_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.unified_pois (\n",
    "            poi_id VARCHAR(50) PRIMARY KEY,\n",
    "            name VARCHAR(200),\n",
    "            layer VARCHAR(100),\n",
    "            district_id VARCHAR(20),\n",
    "            district VARCHAR(100),\n",
    "            neighborhood_id VARCHAR(20),\n",
    "            neighborhood VARCHAR(100),\n",
    "            latitude DECIMAL(9,6),\n",
    "            longitude DECIMAL(9,6),\n",
    "            geometry GEOMETRY, -- changed from geometry to text for testing\n",
    "            attributes JSONB,\n",
    "            nearest_pois JSONB\n",
    "        );\n",
    "    \"\"\"\n",
    "# Runs the above create_unified_table query\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_unified_table))\n",
    "    conn.commit()\n",
    "    print('Unified Table created')\n",
    "\n",
    "# Step 2: Insert all data with nearest_pois computed via CTEs\n",
    "union_queries = []\n",
    "for table in valid_tables:\n",
    "        union_queries.append(f\"\"\"\n",
    "            SELECT \n",
    "                CONCAT(SUBSTRING('{table}' FROM 1 FOR 4), '-', t.id) AS poi_id,\n",
    "                t.name,\n",
    "                '{table}' AS layer,\n",
    "                t.district_id,\n",
    "                t.district,\n",
    "                t.neighborhood_id,\n",
    "                t.neighborhood,\n",
    "                t.latitude,\n",
    "                t.longitude,\n",
    "                ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,\n",
    "                (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes\n",
    "            FROM berlin_source_data.{table} t\n",
    "        \"\"\")\n",
    "    \n",
    "union_sql = \" UNION ALL \".join(union_queries)\n",
    "    \n",
    "insert_sql = f\"\"\"\n",
    "        WITH all_pois AS (\n",
    "            {union_sql}\n",
    "        ),\n",
    "        unique_layers AS (\n",
    "            SELECT DISTINCT layer FROM all_pois WHERE layer <> 'listings'\n",
    "        ),\n",
    "        pois_with_nearest AS (\n",
    "            SELECT \n",
    "                ap.poi_id,\n",
    "                ap.name,\n",
    "                ap.layer,\n",
    "                ap.district_id,\n",
    "                ap.district,\n",
    "                ap.neighborhood_id,\n",
    "                ap.neighborhood,\n",
    "                ap.latitude,\n",
    "                ap.longitude,\n",
    "                ap.geometry,\n",
    "                ap.attributes,\n",
    "                (\n",
    "                    SELECT jsonb_object_agg(layername, nearestinfo)\n",
    "                    FROM (\n",
    "                        SELECT \n",
    "                            ul.layer AS layername,\n",
    "                            (\n",
    "                                SELECT jsonb_build_object(\n",
    "                                    'id', p.poi_id,\n",
    "                                    'name', p.name,\n",
    "                                    'distance', ST_Distance(ap.geometry, p.geometry),\n",
    "                                    'address', jsonb_build_object(\n",
    "                                        'street', p.attributes->>'street',\n",
    "                                        'housenumber', p.attributes->>'housenumber'\n",
    "                                    )\n",
    "                                )\n",
    "                                FROM all_pois p\n",
    "                                WHERE p.layer = ul.layer \n",
    "                                ORDER BY ap.geometry <-> p.geometry\n",
    "                                LIMIT 1\n",
    "                            ) AS nearestinfo\n",
    "                        FROM unique_layers ul\n",
    "                        WHERE ap.layer = 'long_term_listings'\n",
    "                    ) AS layer_nearest\n",
    "                ) AS nearest_pois\n",
    "            FROM all_pois ap\n",
    "        )\n",
    "        INSERT INTO unified_pois \n",
    "            (poi_id, name, layer, district_id, district, neighborhood_id, neighborhood, \n",
    "             latitude, longitude, geometry, attributes, nearest_pois)\n",
    "        SELECT \n",
    "            poi_id, name, layer, district_id, district, neighborhood_id, neighborhood,\n",
    "            latitude, longitude, geometry, attributes, nearest_pois\n",
    "        FROM pois_with_nearest;\n",
    "    \"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(insert_sql))\n",
    "    print(f\"Inserted {len(valid_tables)} tables with computed nearest_pois.\")\n",
    "\n",
    "# Step 3: Create spatial index\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_poi_geom ON unified_pois\n",
    "        USING GIST (geometry);\n",
    "    \"\"\"))\n",
    "    \n",
    "print(\"Spatial index created.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c86384",
   "metadata": {},
   "source": [
    "## Step 3 - Full code including adding tables\n",
    "    - Originally setup to just add layers to the unified table but decided better to recreate the table from scratch each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation query complete\n",
      "✅ Unified table and log table ready\n",
      "✅ New tables to process: ['banks', 'bike_lanes', 'food_markets', 'galleries', 'long_term_listings', 'malls', 'museums']\n",
      "✅ Added 7 new tables to unified_pois.\n",
      "✅Spatial index created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Validation query for excluded tables\n",
    "validation_query = \"\"\"\n",
    "    DROP TABLE IF EXISTS excluded_tables_log CASCADE;\n",
    "\n",
    "    -- Create the excluded_tables_log table\n",
    "    CREATE TABLE public.excluded_tables_log (           -- Adding to public schema for now\n",
    "        table_name VARCHAR(255),\n",
    "        reason VARCHAR(500),\n",
    "        exclusion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "\n",
    "    WITH all_tables AS (\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'berlin_source_data'\n",
    "        AND table_name NOT ILIKE '%districts%'          -- Must not be included in final table so doesnt need to be checked\n",
    "        AND table_name NOT ILIKE '%neighborhoods%'      -- Must not be included in final table so doesnt need to be checked\n",
    "    ),\n",
    "    -- Standard schema for each table\n",
    "    expected_schema AS (\n",
    "        SELECT * FROM (VALUES\n",
    "            ('id', 'character varying', NULL, 'PRIMARY KEY', 'NO'),\n",
    "            ('district_id', 'character varying', NULL, 'FOREIGN KEY', 'NO'),\n",
    "            ('name', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('latitude', 'numeric', NULL, NULL, 'YES'),\n",
    "            ('longitude', 'numeric', NULL, NULL, 'YES'),\n",
    "            ('neighborhood', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('district', 'character varying', NULL, NULL, 'YES'),\n",
    "            ('neighborhood_id', 'character varying', NULL, NULL, 'YES')\n",
    "        ) AS t(column_name, expected_type, expected_length, constraint_type, is_nullable)\n",
    "    ),\n",
    "    -- Checks all columns exist\n",
    "    missing_columns AS (\n",
    "        SELECT tc.table_name,\n",
    "            STRING_AGG(DISTINCT col, ', ' ORDER BY col) AS missing_cols\n",
    "        FROM all_tables tc\n",
    "        CROSS JOIN (VALUES \n",
    "            ('id'), ('name'), ('district_id'), ('district'),\n",
    "            ('latitude'), ('longitude'), ('neighborhood_id'),\n",
    "            ('neighborhood'), ('geometry')\n",
    "        ) AS req_cols(col)\n",
    "        LEFT JOIN information_schema.columns c \n",
    "            ON tc.table_name = c.table_name \n",
    "            AND c.table_schema = 'berlin_source_data'\n",
    "            AND LOWER(c.column_name) = req_cols.col\n",
    "        WHERE c.column_name IS NULL\n",
    "        GROUP BY tc.table_name\n",
    "        HAVING COUNT(*) > 0\n",
    "    ),\n",
    "    -- Checks all data types correct and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "    datatype_issues AS (\n",
    "        SELECT \n",
    "            c.table_name,\n",
    "            'Expected data type ' || e.expected_type || ', got ' || c.data_type AS reason\n",
    "        FROM information_schema.columns c\n",
    "        JOIN expected_schema e \n",
    "            ON LOWER(c.column_name) = e.column_name\n",
    "        WHERE c.table_schema = 'berlin_source_data'\n",
    "        AND (\n",
    "                c.data_type <> e.expected_type\n",
    "                OR (c.data_type = 'numeric' AND (c.numeric_precision || ',' || c.numeric_scale) <> e.expected_length)\n",
    "        )\n",
    "    ),\n",
    "    -- Checks all columns that cannot contain NULL are correct and adds an error to the excluded_tables_log table when incorrect\n",
    "    null_issues AS (\n",
    "        SELECT c.table_name,\n",
    "            'Column ' || c.column_name || ' allows NULL, expected NOT NULL' AS reason\n",
    "        FROM information_schema.columns c\n",
    "        JOIN expected_schema e \n",
    "            ON LOWER(c.column_name) = e.column_name\n",
    "        WHERE c.table_schema = 'berlin_source_data'\n",
    "        AND e.is_nullable = 'NO'\n",
    "        AND c.is_nullable = 'YES'\n",
    "    ),\n",
    "    -- Checks primary key on id column and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "    pk_issues AS (\n",
    "        SELECT t.table_name,\n",
    "            'Missing PRIMARY KEY on id column' AS reason\n",
    "        FROM all_tables t\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "                ON kcu.constraint_name = tc.constraint_name\n",
    "                AND kcu.table_name = tc.table_name\n",
    "            WHERE tc.table_schema = 'berlin_source_data'\n",
    "            AND tc.table_name = t.table_name\n",
    "            AND tc.constraint_type = 'PRIMARY KEY'\n",
    "            AND LOWER(kcu.column_name) = 'id'\n",
    "        )\n",
    "    ),\n",
    "    -- Checks foreign key on district_id column and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "    fk_issues AS (\n",
    "        SELECT t.table_name,\n",
    "            'Missing or incorrect foreign key on district_id' AS reason\n",
    "        FROM all_tables t\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM information_schema.table_constraints tc\n",
    "            WHERE tc.table_schema = 'berlin_source_data'\n",
    "            AND tc.table_name = t.table_name\n",
    "            AND tc.constraint_type = 'FOREIGN KEY'\n",
    "        )\n",
    "    ),\n",
    "    -- Combines all invalid tables into one\n",
    "    invalid_tables AS (\n",
    "        SELECT table_name, 'Missing columns: ' || missing_cols AS reason FROM missing_columns\n",
    "        UNION ALL SELECT table_name, reason FROM datatype_issues\n",
    "        UNION ALL SELECT table_name, reason FROM null_issues\n",
    "        UNION ALL SELECT table_name, reason FROM pk_issues\n",
    "        UNION ALL SELECT table_name, reason FROM fk_issues\n",
    "    )\n",
    "    -- Inserts all invalid tables into excluded_tables_log\n",
    "    INSERT INTO excluded_tables_log (table_name, reason)\n",
    "    SELECT table_name, reason FROM invalid_tables;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(validation_query))\n",
    "print(\"✅ Validation query complete\")\n",
    "\n",
    "# Step 2: Drop unified_pois and processed_tables_log if they exist, then create them again from scratch\n",
    "with engine.begin() as conn:\n",
    "   conn.execute(text(\"DROP TABLE IF EXISTS unified_pois CASCADE;\"))\n",
    "   conn.execute(text(\"DROP TABLE IF EXISTS processed_tables_log CASCADE;\"))\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.unified_pois (                                                                    -- Adding to public schema for now\n",
    "            poi_id VARCHAR(50) PRIMARY KEY,\n",
    "            name VARCHAR(200),\n",
    "            layer VARCHAR(100),\n",
    "            district_id VARCHAR(20),\n",
    "            district VARCHAR(100),\n",
    "            neighborhood_id VARCHAR(20),\n",
    "            neighborhood VARCHAR(100),\n",
    "            latitude DECIMAL(9,6),\n",
    "            longitude DECIMAL(9,6),\n",
    "            geometry GEOMETRY, \n",
    "            attributes JSONB,\n",
    "            nearest_pois JSONB\n",
    "        );\n",
    "    \"\"\"))\n",
    "    conn.execute(text(\"\"\"                                                                                                   \n",
    "        CREATE TABLE IF NOT EXISTS public.processed_tables_log (                                                            -- Adding to public schema for now (Creates log of processed tables)\n",
    "            table_name VARCHAR(255) PRIMARY KEY,\n",
    "            processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\"))\n",
    "print(\"✅ Unified table and log table ready\")\n",
    "\n",
    "# Step 3: Get new tables to process\n",
    "valid_tables_sql = \"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = 'berlin_source_data'\n",
    "        AND table_name NOT ILIKE '%districts%'                                                                                  -- doesn't need to be included in final table\n",
    "        AND table_name NOT ILIKE '%neighborhoods%'                                                                              -- doesn't need to be included in final table\n",
    "        AND table_name NOT IN (SELECT table_name FROM excluded_tables_log)                                                      -- Exclude invalid tables\n",
    "        AND table_name IN ('galleries', 'food_markets','long_term_listings', 'banks', 'bike_lanes', 'malls', 'museums', 'bike_lanes', 'dental_offices', 'bus_tram_stops' )  -- This line is just for testing purposes so can load a few tables at a time \n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    new_tables = [row[0] for row in conn.execute(text(valid_tables_sql))]                                                       # Get list of tables to process\n",
    "\n",
    "print(f\"✅ New tables to process: {new_tables}\")\n",
    "\n",
    "# Step 4: Insert only new tables\n",
    "if new_tables:\n",
    "    union_queries = []                                                                                                          # Reset union_queries for new tables\n",
    "    for table in new_tables:\n",
    "        union_queries.append(f\"\"\"\n",
    "            SELECT \n",
    "                CONCAT(SUBSTRING('{table}' FROM 1 FOR 4), '-', t.id) AS poi_id,                                                   -- Unique poi_id created so no duplicates can happen in error\n",
    "                t.name,\n",
    "                '{table}' AS layer,\n",
    "                t.district_id,\n",
    "                t.district,\n",
    "                t.neighborhood_id,\n",
    "                t.neighborhood,\n",
    "                t.latitude,\n",
    "                t.longitude,\n",
    "                ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                         -- Ensure geometry is in correct SRID\n",
    "                (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
    "            FROM berlin_source_data.{table} t\n",
    "        \"\"\")\n",
    "    union_sql = \" UNION ALL \".join(union_queries)                                                                                   # Combine all SELECTs into one UNION ALL query (one for each valid table)\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "        WITH all_pois AS (\n",
    "            {union_sql}\n",
    "        ),\n",
    "        unique_layers AS (\n",
    "            SELECT DISTINCT layer FROM all_pois WHERE layer <> 'long_term_listings'                                                  -- So that it doesnt add the listing itself to the nearest_pois json doc\n",
    "        ),\n",
    "        pois_with_nearest AS ( \n",
    "            SELECT \n",
    "                ap.poi_id,\n",
    "                ap.name,\n",
    "                ap.layer,\n",
    "                ap.district_id,\n",
    "                ap.district,\n",
    "                ap.neighborhood_id,\n",
    "                ap.neighborhood,\n",
    "                ap.latitude,\n",
    "                ap.longitude,\n",
    "                ap.geometry,\n",
    "                ap.attributes,\n",
    "            (\n",
    "                    SELECT jsonb_object_agg(layername, nearestinfo)                                                                 -- Create jsonb object for nearest pois to the listing\n",
    "                    FROM (\n",
    "                        SELECT \n",
    "                            ul.layer AS layername,\n",
    "                            (\n",
    "                                SELECT jsonb_build_object(\n",
    "                                    'id', p.poi_id,\n",
    "                                    'name', p.name,\n",
    "                                    'distance', ST_Distance(ap.geometry, p.geometry),\n",
    "                                    'address', jsonb_build_object(\n",
    "                                        'street', p.attributes->>'street',\n",
    "                                        'housenumber', p.attributes->>'housenumber'\n",
    "                                    )\n",
    "                                )\n",
    "                                FROM all_pois p\n",
    "                                WHERE p.layer = ul.layer \n",
    "                                ORDER BY ap.geometry <-> p.geometry\n",
    "                                LIMIT 1\n",
    "                            ) AS nearestinfo\n",
    "                        FROM unique_layers ul\n",
    "                        WHERE ap.layer = 'long_term_listings' AND ul.layer <> 'long_term_listings'                                  -- Only add nearest pois for long_term_listings layer\n",
    "                    ) AS layer_nearest\n",
    "                ) AS nearest_pois\n",
    "            FROM all_pois ap\n",
    "        )\n",
    "        INSERT INTO unified_pois \n",
    "            (poi_id, name, layer, district_id, district, neighborhood_id, neighborhood, \n",
    "             latitude, longitude, geometry, attributes, nearest_pois)\n",
    "        SELECT \n",
    "            poi_id, name, layer, district_id, district, neighborhood_id, neighborhood,\n",
    "            latitude, longitude, geometry, attributes, nearest_pois\n",
    "        FROM pois_with_nearest;\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(insert_sql))\n",
    "        for table in new_tables:\n",
    "            conn.execute(text(f\"\"\"\n",
    "                INSERT INTO public.processed_tables_log (table_name)                                                                -- Create a list of tables that have been processed\n",
    "                VALUES ('{table}') ON CONFLICT DO NOTHING;\n",
    "                \"\"\"))\n",
    "                \n",
    "    print(f\"✅ Added {len(new_tables)} new tables to unified_pois.\")\n",
    "else:\n",
    "    print(f\"✅ Insert complete.\")\n",
    "\n",
    "# Step 3: Create spatial index\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_poi_geom ON unified_pois                                                                     -- Create an INDEX so queries run faster on GIST\n",
    "        USING GIST (geometry);\n",
    "    \"\"\"))\n",
    "    \n",
    "print(\"✅Spatial index created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffb6ac",
   "metadata": {},
   "source": [
    "### Check that the table looks correct - including the nearerst_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6614865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bike_lanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food_markets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>galleries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>malls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>banks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>long_term_listings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>museums</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                layer\n",
       "0          bike_lanes\n",
       "1        food_markets\n",
       "2           galleries\n",
       "3               malls\n",
       "4               banks\n",
       "5  long_term_listings\n",
       "6             museums"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT distinct layer\n",
    "    FROM unified_pois  \n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    df= pd.read_sql(text(query), conn)\n",
    "    conn.commit()  # commit the transaction\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561d42b",
   "metadata": {},
   "source": [
    "### Check the nearest_pois is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48d549d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI ID: long-HAU_2950_165_12589\n",
      "{\n",
      "  \"banks\": {\n",
      "    \"id\": \"bank-439234898\",\n",
      "    \"name\": \"santander\",\n",
      "    \"distance\": 0.0832779671080533,\n",
      "    \"address\": {\n",
      "      \"street\": \"b\\u00f6lschestra\\u00dfe\",\n",
      "      \"housenumber\": \"63\"\n",
      "    }\n",
      "  },\n",
      "  \"malls\": {\n",
      "    \"id\": \"mall-26285059\",\n",
      "    \"name\": \"Allende-Center\",\n",
      "    \"distance\": 0.12074375494790712,\n",
      "    \"address\": {\n",
      "      \"street\": \"Pablo-Neruda-Stra\\u00dfe\",\n",
      "      \"housenumber\": \"2-4\"\n",
      "    }\n",
      "  },\n",
      "  \"museums\": {\n",
      "    \"id\": \"muse-738500475\",\n",
      "    \"name\": \"heimatmuseum\",\n",
      "    \"distance\": 0.05624731648576706,\n",
      "    \"address\": {\n",
      "      \"street\": null,\n",
      "      \"housenumber\": null\n",
      "    }\n",
      "  },\n",
      "  \"galleries\": {\n",
      "    \"id\": \"gall-1051272400\",\n",
      "    \"name\": \"ausstellungszentrum pyramide\",\n",
      "    \"distance\": 0.12685044474281532,\n",
      "    \"address\": {\n",
      "      \"street\": \"riesaer stra\\u00dfe\",\n",
      "      \"housenumber\": null\n",
      "    }\n",
      "  },\n",
      "  \"bike_lanes\": {\n",
      "    \"id\": \"bike-way/1161732832\",\n",
      "    \"name\": \"Gr\\u00fcnheider Weg\",\n",
      "    \"distance\": 0.0003777766451616758,\n",
      "    \"address\": {\n",
      "      \"street\": null,\n",
      "      \"housenumber\": null\n",
      "    }\n",
      "  },\n",
      "  \"food_markets\": {\n",
      "    \"id\": \"food-FM098\",\n",
      "    \"name\": \"Flohmarkt Friedrichshagen\",\n",
      "    \"distance\": 0.08314898101047398,\n",
      "    \"address\": {\n",
      "      \"street\": null,\n",
      "      \"housenumber\": null\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT poi_id ,nearest_pois\n",
    "FROM unified_pois  \n",
    ";\n",
    "\"\"\"\n",
    "poi_id_to_check = 'long-HAU_2950_165_12589'\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    df= pd.read_sql(text(query), conn)\n",
    "    conn.commit()  # commit the transaction\n",
    "df\n",
    "\n",
    "# Select a specific POI by poi_id\n",
    "\n",
    "row = df[df['poi_id'] == poi_id_to_check].iloc[0]  # get the first match\n",
    "\n",
    "# Extract the JSON dict\n",
    "nearest_json = row['nearest_pois']  \n",
    "\n",
    "# Only keep id and name for each POI type\n",
    "simplified_json = {\n",
    "    key: {\"id\": value.get(\"id\"), \"name\": value.get(\"name\"), \"distance\": value.get(\"distance\"), \"address\": value.get(\"address\")}\n",
    "    for key, value in nearest_json.items()\n",
    "}\n",
    "\n",
    "# Print\n",
    "print(f\"POI ID: {row['poi_id']}\")\n",
    "print(json.dumps(simplified_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a6541",
   "metadata": {},
   "source": [
    "## Step 4 - Final code\n",
    "    - using only 1 engine.begin() at the start to speed up queries\n",
    "    - removed any unneeded text and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9847b5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation query complete\n",
      "✅ Unified table and log table ready\n",
      "✅ New tables to process: ['banks', 'bus_tram_stops', 'dental_offices', 'food_markets', 'galleries', 'long_term_listings', 'malls', 'museums']\n",
      "✅ Adding banks to unified_pois.\n",
      "✅ Adding bus_tram_stops to unified_pois.\n",
      "✅ Adding dental_offices to unified_pois.\n",
      "✅ Adding food_markets to unified_pois.\n",
      "✅ Adding galleries to unified_pois.\n",
      "✅ Adding long_term_listings to unified_pois.\n",
      "✅ Adding malls to unified_pois.\n",
      "✅ Adding museums to unified_pois.\n",
      "✅ Insert complete.\n",
      "✅Spatial index created.\n"
     ]
    }
   ],
   "source": [
    "with engine.begin() as conn:\n",
    "# Step 1: Validation query for excluded tables\n",
    "    validation_query = \"\"\"\n",
    "        DROP TABLE IF EXISTS excluded_tables_log CASCADE;\n",
    "\n",
    "        -- Create the excluded_tables_log table\n",
    "        CREATE TABLE public.excluded_tables_log (           -- Adding to public schema for now\n",
    "            table_name VARCHAR(255),\n",
    "            reason VARCHAR(500),\n",
    "            exclusion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "\n",
    "        WITH all_tables AS (\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = 'berlin_source_data'\n",
    "            AND table_name NOT ILIKE '%districts%'          -- Must not be included in final table so doesnt need to be checked\n",
    "            AND table_name NOT ILIKE '%neighborhoods%'      -- Must not be included in final table so doesnt need to be checked\n",
    "        ),\n",
    "        -- Standard schema for each table\n",
    "        expected_schema AS (\n",
    "            SELECT * FROM (VALUES\n",
    "                ('id', 'character varying', NULL, 'PRIMARY KEY', 'NO'),\n",
    "                ('district_id', 'character varying', NULL, 'FOREIGN KEY', 'NO'),\n",
    "                ('name', 'character varying', NULL, NULL, 'YES'),\n",
    "                ('latitude', 'numeric', NULL, NULL, 'YES'),\n",
    "                ('longitude', 'numeric', NULL, NULL, 'YES'),\n",
    "                ('neighborhood', 'character varying', NULL, NULL, 'YES'),\n",
    "                ('district', 'character varying', NULL, NULL, 'YES'),\n",
    "                ('neighborhood_id', 'character varying', NULL, NULL, 'YES')\n",
    "            ) AS t(column_name, expected_type, expected_length, constraint_type, is_nullable)\n",
    "        ),\n",
    "        -- Checks all columns exist\n",
    "        missing_columns AS (\n",
    "            SELECT tc.table_name,\n",
    "                STRING_AGG(DISTINCT col, ', ' ORDER BY col) AS missing_cols\n",
    "            FROM all_tables tc\n",
    "            CROSS JOIN (VALUES \n",
    "                ('id'), ('name'), ('district_id'), ('district'),\n",
    "                ('latitude'), ('longitude'), ('neighborhood_id'),\n",
    "                ('neighborhood'), ('geometry')\n",
    "            ) AS req_cols(col)\n",
    "            LEFT JOIN information_schema.columns c \n",
    "                ON tc.table_name = c.table_name \n",
    "                AND c.table_schema = 'berlin_source_data'\n",
    "                AND LOWER(c.column_name) = req_cols.col\n",
    "            WHERE c.column_name IS NULL\n",
    "            GROUP BY tc.table_name\n",
    "            HAVING COUNT(*) > 0\n",
    "        ),\n",
    "        -- Checks all data types correct and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "        datatype_issues AS (\n",
    "            SELECT \n",
    "                c.table_name,\n",
    "                'Expected data type ' || e.expected_type || ', got ' || c.data_type AS reason\n",
    "            FROM information_schema.columns c\n",
    "            JOIN expected_schema e \n",
    "                ON LOWER(c.column_name) = e.column_name\n",
    "            WHERE c.table_schema = 'berlin_source_data'\n",
    "            AND (\n",
    "                    c.data_type <> e.expected_type\n",
    "                    OR (c.data_type = 'numeric' AND (c.numeric_precision || ',' || c.numeric_scale) <> e.expected_length)\n",
    "            )\n",
    "        ),\n",
    "        -- Checks all columns that cannot contain NULL are correct and adds an error to the excluded_tables_log table when incorrect\n",
    "        null_issues AS (\n",
    "            SELECT c.table_name,\n",
    "                'Column ' || c.column_name || ' allows NULL, expected NOT NULL' AS reason\n",
    "            FROM information_schema.columns c\n",
    "            JOIN expected_schema e \n",
    "                ON LOWER(c.column_name) = e.column_name\n",
    "            WHERE c.table_schema = 'berlin_source_data'\n",
    "            AND e.is_nullable = 'NO'\n",
    "            AND c.is_nullable = 'YES'\n",
    "        ),\n",
    "        -- Checks primary key on id column and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "        pk_issues AS (\n",
    "            SELECT t.table_name,\n",
    "                'Missing PRIMARY KEY on id column' AS reason\n",
    "            FROM all_tables t\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM information_schema.table_constraints tc\n",
    "                JOIN information_schema.key_column_usage kcu\n",
    "                    ON kcu.constraint_name = tc.constraint_name\n",
    "                    AND kcu.table_name = tc.table_name\n",
    "                WHERE tc.table_schema = 'berlin_source_data'\n",
    "                AND tc.table_name = t.table_name\n",
    "                AND tc.constraint_type = 'PRIMARY KEY'\n",
    "                AND LOWER(kcu.column_name) = 'id'\n",
    "            )\n",
    "        ),\n",
    "        -- Checks foreign key on district_id column and creates and adds an error to the excluded_tables_log table when incorrect\n",
    "        fk_issues AS (\n",
    "            SELECT t.table_name,\n",
    "                'Missing or incorrect foreign key on district_id' AS reason\n",
    "            FROM all_tables t\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM information_schema.table_constraints tc\n",
    "                WHERE tc.table_schema = 'berlin_source_data'\n",
    "                AND tc.table_name = t.table_name\n",
    "                AND tc.constraint_type = 'FOREIGN KEY'\n",
    "            )\n",
    "        ),\n",
    "        -- Combines all invalid tables into one\n",
    "        invalid_tables AS (\n",
    "            SELECT table_name, 'Missing columns: ' || missing_cols AS reason FROM missing_columns\n",
    "            UNION ALL SELECT table_name, reason FROM datatype_issues\n",
    "            UNION ALL SELECT table_name, reason FROM null_issues\n",
    "            UNION ALL SELECT table_name, reason FROM pk_issues\n",
    "            UNION ALL SELECT table_name, reason FROM fk_issues\n",
    "        )\n",
    "        -- Inserts all invalid tables into excluded_tables_log\n",
    "        INSERT INTO excluded_tables_log (table_name, reason)\n",
    "        SELECT table_name, reason FROM invalid_tables;\n",
    "\"\"\"\n",
    "\n",
    "    conn.execute(text(validation_query))\n",
    "    print(\"✅ Validation query complete\")\n",
    "\n",
    "# Step 2: Drop unified_pois and processed_tables_log if they exist, then create them again from scratch\n",
    "\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS unified_pois CASCADE;\"))\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS processed_tables_log CASCADE;\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS public.unified_pois (                                                                    -- Adding to public schema for now\n",
    "            poi_id VARCHAR(50) PRIMARY KEY,\n",
    "            name VARCHAR(200),\n",
    "            layer VARCHAR(100),\n",
    "            district_id VARCHAR(20),\n",
    "            district VARCHAR(100),\n",
    "            neighborhood_id VARCHAR(20),\n",
    "            neighborhood VARCHAR(100),\n",
    "            latitude DECIMAL(9,6),\n",
    "            longitude DECIMAL(9,6),\n",
    "            geometry GEOMETRY, \n",
    "            attributes JSONB,\n",
    "            nearest_pois JSONB\n",
    "        );\n",
    "    \"\"\"))\n",
    "    conn.execute(text(\"\"\"                                                                                                   \n",
    "        CREATE TABLE IF NOT EXISTS public.processed_tables_log (                                                            -- Adding to public schema for now (Creates log of processed tables)\n",
    "            table_name VARCHAR(255) PRIMARY KEY,\n",
    "            processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\"))\n",
    "    print(\"✅ Unified table and log table ready\")\n",
    "\n",
    "    # Step 3: Get new tables to process\n",
    "    valid_tables_sql = \"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'berlin_source_data'\n",
    "            AND table_name NOT ILIKE '%districts%'                                                                          -- doesn't need to be included in final table\n",
    "            AND table_name NOT ILIKE '%neighborhoods%'                                                                      -- doesn't need to be included in final table\n",
    "            AND table_name NOT IN (SELECT table_name FROM excluded_tables_log)                                              -- Exclude invalid tables\n",
    "            AND table_name IN ('galleries', 'food_markets','long_term_listings', 'banks', 'malls', 'museums', 'dental_offices', 'bus_tram_stops' )  -- This line is just for testing purposes so can load a few tables at a time , 'bike_lanes'\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "    new_tables = [row[0] for row in conn.execute(text(valid_tables_sql))]                                                    # Get list of tables to process\n",
    "    print(f\"✅ New tables to process: {new_tables}\")\n",
    "\n",
    "    # Step 4: Insert only new tables\n",
    "    if new_tables:\n",
    "        union_queries = []                                                                                                   # Reset union_queries for new tables\n",
    "        for table in new_tables:\n",
    "            union_queries.append(f\"\"\"\n",
    "                SELECT \n",
    "                    CONCAT(SUBSTRING('{table}' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
    "                    t.name,\n",
    "                    '{table}' AS layer,\n",
    "                    t.district_id,\n",
    "                    t.district,\n",
    "                    t.neighborhood_id,\n",
    "                    t.neighborhood,\n",
    "                    t.latitude,\n",
    "                    t.longitude,\n",
    "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
    "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
    "                FROM berlin_source_data.{table} t\n",
    "            \"\"\")\n",
    "        union_sql = \" UNION ALL \".join(union_queries)                                                                          # Combine all SELECTs into one UNION ALL query (one for each valid table)\n",
    "\n",
    "        insert_sql = f\"\"\"\n",
    "            WITH all_pois AS (\n",
    "                {union_sql}\n",
    "            ),\n",
    "            unique_layers AS (\n",
    "                SELECT DISTINCT layer FROM all_pois WHERE layer <> 'long_term_listings'                                          -- Won't add the listing itself to the nearest_pois json doc\n",
    "            ),\n",
    "            pois_with_nearest AS ( \n",
    "                SELECT \n",
    "                    ap.poi_id,\n",
    "                    ap.name,\n",
    "                    ap.layer,\n",
    "                    ap.district_id,\n",
    "                    ap.district,\n",
    "                    ap.neighborhood_id,\n",
    "                    ap.neighborhood,\n",
    "                    ap.latitude,\n",
    "                    ap.longitude,\n",
    "                    ap.geometry,\n",
    "                    ap.attributes,\n",
    "                (\n",
    "                    SELECT jsonb_object_agg(layername, nearestinfo)                                                               -- Create jsonb object for nearest pois to the listing\n",
    "                    FROM (\n",
    "                        SELECT \n",
    "                            ul.layer AS layername,\n",
    "                            (\n",
    "                                SELECT jsonb_build_object(\n",
    "                                    'id', p.poi_id,\n",
    "                                    'name', p.name,\n",
    "                                    'distance', ST_Distance(ap.geometry, p.geometry),\n",
    "                                    'address', jsonb_build_object(\n",
    "                                        'street', p.attributes->>'street',\n",
    "                                        'housenumber', p.attributes->>'housenumber'\n",
    "                                    )\n",
    "                                )\n",
    "                                FROM all_pois p\n",
    "                                WHERE p.layer = ul.layer \n",
    "                                ORDER BY ap.geometry <-> p.geometry\n",
    "                                LIMIT 1\n",
    "                            ) AS nearestinfo\n",
    "                        FROM unique_layers ul\n",
    "                        WHERE ap.layer = 'long_term_listings' AND ul.layer <> 'long_term_listings'                                -- Only add nearest pois for long_term_listings layer\n",
    "                    ) AS layer_nearest\n",
    "                ) AS nearest_pois\n",
    "            FROM all_pois ap\n",
    "        )\n",
    "        INSERT INTO unified_pois \n",
    "            (poi_id, name, layer, district_id, district, neighborhood_id, neighborhood, \n",
    "             latitude, longitude, geometry, attributes, nearest_pois)\n",
    "        SELECT \n",
    "            poi_id, name, layer, district_id, district, neighborhood_id, neighborhood,\n",
    "            latitude, longitude, geometry, attributes, nearest_pois\n",
    "        FROM pois_with_nearest;\n",
    "    \"\"\"\n",
    "    \n",
    "    conn.execute(text(insert_sql))\n",
    "\n",
    "    for table in new_tables:\n",
    "        conn.execute(text(f\"\"\"\n",
    "            INSERT INTO public.processed_tables_log (table_name)                                                                    -- Create a list of tables that have been processed\n",
    "            VALUES ('{table}') ON CONFLICT DO NOTHING;\n",
    "        \"\"\"))\n",
    "                    \n",
    "        print(f\"✅ Adding {table} to unified_pois.\")\n",
    "    else:\n",
    "        print(f\"✅ Insert complete.\")\n",
    "\n",
    "    # Step 3: Create spatial index\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_poi_geom ON unified_pois                                                                   -- Create an INDEX so queries run faster on GIST\n",
    "        USING GIST (geometry);\n",
    "    \"\"\"))\n",
    "        \n",
    "    print(\"✅Spatial index created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9ad96",
   "metadata": {},
   "source": [
    "## Show excluded table logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "523453da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>reason</th>\n",
       "      <th>exclusion_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parking_spaces</td>\n",
       "      <td>Missing columns: latitude, longitude</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veterinary_clinics</td>\n",
       "      <td>Column district_id allows NULL, expected NOT NULL</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schools</td>\n",
       "      <td>Column district_id allows NULL, expected NOT NULL</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           table_name                                             reason  \\\n",
       "0      parking_spaces               Missing columns: latitude, longitude   \n",
       "1  veterinary_clinics  Column district_id allows NULL, expected NOT NULL   \n",
       "2             schools  Column district_id allows NULL, expected NOT NULL   \n",
       "\n",
       "              exclusion_date  \n",
       "0 2025-11-26 11:28:11.275156  \n",
       "1 2025-11-26 11:28:11.275156  \n",
       "2 2025-11-26 11:28:11.275156  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM excluded_tables_log\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    df= pd.read_sql(text(query), conn)\n",
    "    conn.commit()  # commit the transaction\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3531a",
   "metadata": {},
   "source": [
    "### Notes on the missing data\n",
    "\n",
    "- parking_spaces has many types of geometries, maybe we can ask Dido to adjust her script as looking at it scares me\n",
    "- schools - district_id column has NULLS so cant add constraint\n",
    "- vets - district_id column has NULLS so cant add constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7918f2",
   "metadata": {},
   "source": [
    "## Show processed table logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1bfb73ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>processed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banks</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bus_tram_stops</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dental_offices</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food_markets</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>galleries</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>long_term_listings</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>malls</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>museums</td>\n",
       "      <td>2025-11-26 11:28:11.275156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           table_name             processed_date\n",
       "0               banks 2025-11-26 11:28:11.275156\n",
       "1      bus_tram_stops 2025-11-26 11:28:11.275156\n",
       "2      dental_offices 2025-11-26 11:28:11.275156\n",
       "3        food_markets 2025-11-26 11:28:11.275156\n",
       "4           galleries 2025-11-26 11:28:11.275156\n",
       "5  long_term_listings 2025-11-26 11:28:11.275156\n",
       "6               malls 2025-11-26 11:28:11.275156\n",
       "7             museums 2025-11-26 11:28:11.275156"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM processed_tables_log\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    df= pd.read_sql(text(query), conn)\n",
    "    conn.commit()  # commit the transaction\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503917f",
   "metadata": {},
   "source": [
    "### Example of what the Union query looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04edfa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('banks' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'banks' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.banks t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('bus_tram_stops' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'bus_tram_stops' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.bus_tram_stops t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('dental_offices' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'dental_offices' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.dental_offices t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('food_markets' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'food_markets' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.food_markets t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('galleries' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'galleries' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.galleries t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('long_term_listings' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'long_term_listings' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.long_term_listings t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('malls' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'malls' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.malls t\n",
      "             UNION ALL \n",
      "                SELECT \n",
      "                    CONCAT(SUBSTRING('museums' FROM 1 FOR 4), '-', t.id) AS poi_id,                                          -- Unique poi_id created so no duplicates can happen in error\n",
      "                    t.name,\n",
      "                    'museums' AS layer,\n",
      "                    t.district_id,\n",
      "                    t.district,\n",
      "                    t.neighborhood_id,\n",
      "                    t.neighborhood,\n",
      "                    t.latitude,\n",
      "                    t.longitude,\n",
      "                    ST_SetSRID(ST_GeomFromEWKT(t.geometry), 4326) AS geometry,                                                -- Ensure geometry is in correct SRID\n",
      "                    (to_jsonb(t) - 'id' - 'name' - 'district_id' - 'neighborhood_id' - 'latitude' - 'longitude' - 'geometry') AS attributes     -- All other columns as attributes in JSONB\n",
      "                FROM berlin_source_data.museums t\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(union_sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
